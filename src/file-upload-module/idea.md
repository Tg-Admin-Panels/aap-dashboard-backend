Bulk Upload FeatureScope: Implement a robust, scalable bulk upload mechanism for CSV/XLSX files (typical 23–30 MB, up to ~1M rows) using Direct Upload → Server-Side Streaming Parse → Batch Insert → SSE Progress with optional background workers for concurrency.1) Goals & Non‑Functional RequirementsPrimary GoalsUpload medium‑large files (20–50 MB) reliably.Stream‑parse to avoid memory spikes.Insert into MongoDB at high throughput with accurate, real‑time progress in the UI.Support multiple concurrent uploads with graceful queuing and isolation.UX‑Level Benchmarks / SLAsUpload UIProgress bar visible; UI never freezes.Upload time (23–30 MB): 10–30 sec on typical office networks.Processing UILive status via SSE: queued → parsing → validating → inserting → indexing → completed.3 lakh rows: 2–4 min; 5 lakh rows: 3–6 min; 1M rows: 6–12 min (CSV faster; XLSX slower).Never exceed 10–12 min for 1M rows on the recommended infra; otherwise auto‑flag performance alert.CompletionClear success message; data visible in app within ≤30 sec after completion.Downloadable error report for failed rows.System‑Level BenchmarksParsing throughput (CSV streaming): 100k–300k rows/min.Insert throughput (batch 5k–10k): 3k–10k docs/sec depending on schema/indexes.Peak RAM during processing (per job): ≤500 MB for CSV; ≤1 GB for XLSX streams.App server p99 request latency unaffected (processing offloaded to workers).Reliability & SafetyAt‑least‑once ingestion semantics with idempotent upserts (optional).Graceful retries for transient DB or FS errors.Quotas / rate limits to prevent overload.2) Architecture OverviewHigh‑Level FlowClient uploads file (multipart/form‑data) → API Server stores temp file, returns jobId.API enqueues jobId to Queue (BullMQ/Redis) and immediately responds.Worker picks up the job, stream‑parses temp file (CSV or XLSX), validates/transforms rows, does batch inserts to MongoDB.Worker emits progress events to SSE hub keyed by jobId.Client subscribes to SSE (/uploads/:jobId/events) and shows progress; on completion shows summary + error report link.Browser ──(multipart)──> API ──(enqueue)──> Redis/BullMQ ──> Worker ──> MongoDB │                               │                                 ▲ └──────────────(SSE)────────────┴────────────────────(progress)────┘ComponentsAPI Server (Express/Node): file intake, job creation, SSE endpoint.Worker(s): CPU/IO heavy (parse + insert). Horizontal scale.Redis: BullMQ queue broker. (Agenda is acceptable if Redis unavailable.)MongoDB: Primary datastore (separate instance recommended).Object Storage (optional): S3/GCS for temp files instead of local disk if running in containers.3) Detailed Design3.1 EndpointsPOST /api/v1/uploadsAuth required.Accepts file (multipart), datasetType (optional), parseOptions (optional).Response: { jobId }.GET /api/v1/uploads/:jobId/events (SSE)Streams events: { status, processedRows, totalRows, percent, message, errorReportUrl }.GET /api/v1/uploads/:jobId (Job status)Returns latest status JSON (for fallbacks / tests).GET /api/v1/uploads/:jobId/error-reportStreams/downloads CSV of failed rows.3.2 File Intake (API)Use busboy or multer (stream mode) to write the incoming file directly to disk/S3: uploads/<jobId>.<ext>.Basic validations pre‑parse:Extension whitelist: .csv, .xlsx.Size limit (e.g., 60 MB) with friendly error.Enqueue job with { jobId, path, ext, userId, schemaVersion, createdAt }.3.3 Worker Parse & InsertRow Counting for ProgressCSV: quick pre‑scan to count lines (cheap); or estimate incrementally and backfill total.XLSX: use workbook reader; if no reliable total, emit progress by processed count and show spinner for total.ParsingCSV: fast-csv or csv-parser in stream mode with headers: true.XLSX: exceljs streaming reader (WorkbookReader({ sharedStrings: false })).Transform/ValidateMap headers → canonical fields.Validate types (dates, numbers), normalise enums; collect errors.Batching StrategyAccumulate 5k–10k docs, then insertMany(batch, { ordered: false }).Optional upsert: bulkWrite([{ updateOne: { filter, update, upsert: true } }...]) when idempotency required.Indexing StrategyDisable heavy secondary indexes during ingest; recreate after job completes (or use background: true / hidden indexes).Progress EmissionAfter each batch: push { processedRows, totalRows, percent, status: "inserting" } to SSE.Error ReportingOn row‑level failures, write to uploads/<jobId>-errors.csv.Final SSE includes errorReportUrl if any failures.CleanupRemove temp file on success/failure (or lifecycle policy on S3).3.4 SSE Implementation NotesKeep one SSE connection per jobId.Heartbeat every 15–30s to keep connections alive.Auto‑reconnect on client.Event schema:{ "jobId": "...", "status": "queued|parsing|validating|inserting|indexing|completed|failed", "processedRows": 123456, "totalRows": 500000, "percent": 24.6, "message": "optional human text", "errorReportUrl": "optional" }4) Data Model & ValidationCollection: records (example)Schema design principlesAvoid deeply nested subdocuments if not needed; flatten for insert speed.Pre‑compute frequently queried fields.Keep document size < 1 MB (Mongo limit is 16 MB; aim far lower).ValidationLightweight sync validation in worker (type checks, required fields).Heavy validation deferred as asynchronous “post‑ingest checks”.5) Security & ComplianceAuthN/AuthZ on all endpoints; per‑tenant isolation via tenantId scoped collections or filters.Server‑side size cap (e.g., 60 MB), extension whitelist, content sniffing.Virus/malware scan hook (optional) before queueing.Least‑privilege IAM for object storage.Encrypted at rest (disk/S3) and TLS in transit.PII handling: redact in logs; store error reports securely with expiring URLs.6) Observability & OperationsStructured Logging: job lifecycle logs with jobId, tenantId, timings per stage.Metrics (Prometheus/OpenTelemetry):upload_time_seconds, parse_time_seconds, insert_time_seconds.rows_processed_total, rows_per_second, batches_total, failures_total.Worker CPU/RAM, Mongo insert latency, queue depth.AlertsJob > SLA threshold (e.g., >10 min for 1M CSV).Error rate >2% rows.Queue depth > N for > M minutes.7) Capacity Planning / SizingRecommended Production BaselineAPI Server: 4 vCPU, 8 GB RAM, 100 GB SSD.Worker: 4 vCPU, 8–16 GB RAM, 100 GB SSD (start with 1; scale to 2–3 for concurrency).MongoDB: 4 vCPU, 16 GB RAM, SSD ≥1000 IOPS (separate instance).Redis: Small (1–2 vCPU, 2–4 GB RAM).Concurrency Expectations1–3 concurrent large uploads: OK on baseline.5–10 concurrent: add workers (one worker ≈ 1–2 heavy jobs concurrently) and ensure Mongo IOPS/RAM are sufficient.8) Benchmarks & Acceptance CriteriaTarget BenchmarksFileRowsFormatUploadParse+InsertTotalNotes23 MB300kCSV10–20 s2–4 min3–5 minPreferred path30 MB500kCSV15–25 s3–6 min4–7 min5k–10k batch24 MB1,000kXLSX15–30 s6–10 min7–12 minXLSX slowerUX AcceptanceProgress bar during upload; status stream during processing.Final toast/dialog with counts: inserted, skipped, failed + download error report.UI responsive and navigable during processing.Functional AcceptanceCorrect mapping/validation per spec.Idempotent re‑run (optional): reuploading same file doesn’t duplicate rows when a unique key is configured.Error report contains row number, error reason, and original payload.9) Fallbacks & Edge CasesVery Large/Unreliable Networks: switch to chunked/resumable uploads (tus.io or custom chunks) behind a feature flag.CSV Dialect Variants: auto‑detect delimiter, quote, encoding (UTF‑8/BOM).Header Mismatch: fail fast with preview + mapping UI.Duplicate Handling: configurable unique key (e.g., externalId).Partial Failures: continue unordered inserts; report failures.Cancellation: user can cancel job → worker aborts parse and cleans up.10) Implementation Blueprint (Pseudocode)API: Upload// POST /api/v1/uploads app.post('/api/v1/uploads', auth, upload.single('file'), async (req, res) => { const jobId = uuid(); const { path, originalname } = req.file; // multer/busboy path await queue.add('ingest', { jobId, path, originalname, userId: req.user.id }); res.json({ jobId }); });SSE Endpointapp.get('/api/v1/uploads/:jobId/events', auth, sseMiddleware, (req, res) => { sseHub.subscribe(req.params.jobId, res); });Worker`queue.process('ingest', async (job) => { const { jobId, path, originalname } = job.data; const parser = pickParserByExt(originalname); // csv or xlsx let processed = 0, total = await estimateTotalRows(path, originalname); let batch = [];for await (const row of parser.iterRows(path)) { const doc = transform(row); const { ok, err } = validate(doc); if (!ok) { errorSink.write(row, err); continue; } batch.push(doc);if (batch.length >= BATCH_SIZE) {
  await MyModel.insertMany(batch, { ordered: false });
  processed += batch.length; batch = [];
  sseHub.emit(jobId, { status: 'inserting', processedRows: processed, totalRows: total, percent: calc(processed,total) });
}} if (batch.length) { await MyModel.insertMany(batch, { ordered: false }); processed += batch.length; } sseHub.emit(jobId, { status: 'completed', processedRows: processed, totalRows: total, percent: 100, errorReportUrl }); cleanup(path); }); `11) Testing & Benchmark PlanUnit: parsers (CSV/XLSX), validators, transformers.Integration: end‑to‑end ingest on sample files: 100k, 300k, 500k, 1M rows.Load: simulate N concurrent jobs (k6/Artillery) with real files; measure rows/sec and total time.Failure Injection: network blips, Mongo transient errors, malformed rows.User Acceptance: verify UI/UX flow, SSE updates, error report download.12) Timeline (Indicative)Week 1: API upload + basic worker with CSV; SSE; simple UI wiring.Week 2: XLSX streaming, validation/transforms, error reporting, metrics.Week 3: Benchmarks, index strategy, idempotency option, polish UX.Week 4: Load tests, hardening, docs, rollout.13) Risks & MitigationsMongo bottleneck under concurrency → limit concurrent workers; increase batch size carefully; add RAM/IOPS.Memory leaks in streams → rigorous backpressure handling; use for await iterators; finalize streams.SSE disconnects → client auto‑reconnect; server heartbeat; resume from last emitted progress.XLSX oddities → prefer CSV when possible; robust library versions pinned.14) DeliverablesWorking feature with endpoints documented (OpenAPI/Swagger).React UI with upload + progress + results.Worker service with parsers, validators, batch insert, error reporting.Metrics dashboards + alerts.Runbooks for on‑call (retry stuck jobs, clean temp files, regenerate indexes).15) Configuration DefaultsBATCH_SIZE=5000 MAX_FILE_SIZE_MB=60 SSE_HEARTBEAT_MS=20000 QUEUE_CONCURRENCY=2 UPLOAD_STORAGE=/data/uploads ERROR_REPORT_TTL_DAYS=7 INDEX_REBUILD=true CSV_DELIM=auto